{"cells":[{"cell_type":"markdown","source":["## Large Scale Machine Learning Made Easy using Azure Databricks with Snowflake\nHello! This is a demonstration of how you can use Azure Databricks to build a recommendation algorithm from your Snowflake database tables. We use the publicly available Netflix movies data, a record of 100 million movie ratings by Netflix users. This is publicly available data from the [Kaggle website](https://www.kaggle.com/netflix-inc/netflix-prize-data). \n\nFirst we explore the dataset, showing the capability to use open source python packages ggplot and seaborn within the notebook. Then the PySpark package pyspark.ml is used to train the recommendation algorithm and is applied to the whole dataset. This will provide 3 movie recommendations for each user based on their ratings so far. Then we will export these recommendations back to the Snowflake database.\n\n*NB: cluster specs: Azure Standard_DS3_v2 with 2 worker nodes and autoscaling enabled. Total run time for different sample sizes: 5mins for 1m, 9mins for 10m, 20min for 100m (full dataset)*"],"metadata":{}},{"cell_type":"markdown","source":["#### Extract the data from the Snowflake database using the spark-snowflake connector package. \n*NB: you will need to attach the snowflake-spark connector package to your cluster before you are able to execute read/write commands with the Snowflake database.*"],"metadata":{}},{"cell_type":"code","source":["sfOptions = {\n  \"sfURL\" : \"your.sf.portal.url.com\",\n  \"sfAccount\" : \"yourAccount\",\n  \"sfUser\" : \"USER\",\n  \"sfPassword\" : \"PWD\",\n  \"sfDatabase\" : \"YOU_DB\",\n  \"sfSchema\" : \"PUBLIC\",\n  \"sfWarehouse\" : \"YOUR_WH\",\n}\nSNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n\n#full netflix ratings dataset\ndf = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n  .options(**sfOptions) \\\n  .option(\"query\",  \"select * from netflix_ratings_wide\") \\\n  .load()\n\n#movie title reference table\nmovie_titles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n  .options(**sfOptions) \\\n  .option(\"query\",  \"select MOVIE_ID, MOVIE_TITLE from netflix_movie_titles\") \\\n  .load()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df_num_rows = df.count()\nprint('Number of datapoints: ', df_num_rows)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Data exploration with ggplot and seaborn\nIn any machine experiment an essential part is gaining an understanding of the data, usually by producing data analysis and visualisations. This can help find any patterns or artefacts of the data.\nUsing display(df) and clicking the bar chart button beneath the cell is an easy way to view plots of a sample of your data. For anything beyond that, it is best to use python pacakges like ggplot and seaborn to create nicer, static visualisations of data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import * \nfrom pyspark.sql.types import *\nmovie_rating_stats = df.groupby('MOVIE_TITLE').agg(mean(\"RATING\"), stddev(\"RATING\")).withColumnRenamed('avg(RATING)', \"Mean Rating\").withColumnRenamed('stddev_samp(RATING)', 'Std Rating')\nmovie_rating_stats = movie_rating_stats.withColumn(\"Mean Rating\",movie_rating_stats[\"Mean Rating\"].cast(FloatType()))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Distribution of average movie ratings"],"metadata":{}},{"cell_type":"code","source":["from ggplot import *\nimport seaborn as sns\nm_ratings_plot = ggplot(movie_rating_stats.toPandas(), aes('Mean Rating')) + geom_bar() + ylab('Frequency') +ggtitle('Distribution of Average Movie Ratings') + xlim(1,5)\ndisplay(m_ratings_plot)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["#### A few movies' ratings distributions: boxplots showing mean and inter-quartile range"],"metadata":{}},{"cell_type":"code","source":["movie_sample_size = 6/df_num_rows\nsample_movies = list(df.sample(False, movie_sample_size, 4).toPandas()['MOVIE_TITLE'].unique())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import seaborn as sns\nsns.set(font_scale = 0.7)\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax = sns.boxplot(x=\"MOVIE_TITLE\", y=\"RATING\", data = df.filter(df.MOVIE_TITLE.isin(sample_movies)).withColumn(\"RATING\",df[\"RATING\"].cast(FloatType())).toPandas())\nax.set(xlabel = '')\nplt.xticks(rotation=10)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["##### Movie ratings by year of creation"],"metadata":{}},{"cell_type":"code","source":["yearly_ratings = df.groupby('YEAR').mean().select(['YEAR', 'avg(RATING)']).withColumnRenamed('avg(RATING)', 'Mean Rating')"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["m_ratings_dist = ggplot(yearly_ratings.withColumn(\"Mean Rating\",yearly_ratings[\"Mean Rating\"].cast(FloatType())).toPandas(), aes(x = 'YEAR', y = 'Mean Rating')) + geom_smooth(level = 0.9) + xlim(1890, 2010) + ylim(2,4) + xlab('Year') + ylab('Average Movie Rating')\ndisplay(m_ratings_dist)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##### User ratings distribution sample"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nuser_sample_size = 4/df_num_rows\nsample_users = list(df.sample(False, user_sample_size, 4).select('USER_ID').toPandas()['USER_ID'].unique())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["fig, ax = plt.subplots()\nax = sns.violinplot(x=\"USER_ID\", y=\"RATING\", data = df.filter(df.USER_ID.isin(sample_users)).withColumn(\"RATING\",df[\"RATING\"].cast(FloatType())).toPandas(), inner = None)\nax = sns.stripplot(x=\"USER_ID\", y=\"RATING\", data = df.filter(df.USER_ID.isin(sample_users)).withColumn(\"RATING\",df[\"RATING\"].cast(FloatType())).toPandas(), jitter = True, color = \".3\")\nplt.xticks(rotation=10)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["You could take this exploration much further - examining correlations, grouping users, etc. For succinctness we will move on to the model building now.\n### The model \nWe build a model which recommends three movies to users based on the users' historical ratings.\n\nThe training of the model only takes two lines of code. The first initiates the parameterisation of the model, and the second fits it. The model is a Spark optimised version of the Alternating Least Squares (ALS) algorithm. This uses clever matrix factorization techniques to build recommendations. We specify the 'userCol', 'itemCol' and 'ratingCol' to let the underlying functions fit to our purpose. The 'coldStartStrategy' argument specifies how we handle unseen users/movies when it comes to recommendations. For simplicity, these cases are dropped from model training. An additional method is used on the fitted model to create the 'recommendations' DataFrame which gives us 3 movie recommendations for each user."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nals = ALS(maxIter=10, regParam=0.1, userCol = 'USER_ID', itemCol='MOVIE_ID', ratingCol='RATING', coldStartStrategy = \"drop\")\nmodel = als.fit(df)\n#Create recommendations from the model\nrecommendations = model.recommendForAllUsers(3)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["##### Reshape the data into something to export back to Snowflake"],"metadata":{}},{"cell_type":"code","source":["flat_recs = recommendations.select(\"USER_ID\", recommendations.recommendations[0]['MOVIE_ID'].alias('REC1_MOVIE_ID'), recommendations.recommendations[1]['MOVIE_ID'].alias('REC2_MOVIE_ID'), recommendations.recommendations[2]['MOVIE_ID'].alias('REC3_MOVIE_ID'))\nfinal_recs = flat_recs.join(movie_titles, (movie_titles.MOVIE_ID == flat_recs.REC1_MOVIE_ID), how = 'inner').drop('MOVIE_ID').withColumnRenamed('MOVIE_TITLE', 'REC1_MOVIE_TITLE').join(movie_titles, (movie_titles.MOVIE_ID == flat_recs.REC2_MOVIE_ID)).drop('MOVIE_ID').withColumnRenamed('MOVIE_TITLE', 'REC2_MOVIE_TITLE').join(movie_titles, (movie_titles.MOVIE_ID == flat_recs.REC3_MOVIE_ID)).drop('MOVIE_ID').withColumnRenamed('MOVIE_TITLE', 'REC3_MOVIE_TITLE')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### The recommendations\n\nFinally - explore the recommendations you've built. You may recognise some movies - and see some similar movies being recommended to a user. This means it's doing what we expect it to do. Another sanity check would be to take a sample of a few users' highest rated movies - and see if they seem similar to the ones being recommended to them. We'll leave that for today and show you how to load the results table into Snowflake."],"metadata":{}},{"cell_type":"code","source":["display(final_recs)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Load to Snowflake as a new table in your database"],"metadata":{}},{"cell_type":"code","source":["final_recs.write.format(\"net.snowflake.spark.snowflake\").options(**sfOptions).option('dbtable', 'NETFLIX_RECOMMENDATIONS').mode('overwrite').saveAsTable('NETFLIX_RECOMMENDATIONS')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### Conclusion\n\nWith Snowflake as the data store and Azure Databricks as development environment powered by a Spark cluster, there is no limit for your machine learning experiments. Load and explore large datasets in the same way you would small. Use the open source machine learning libraries available in PySpark to train optimised models to your use case in only a few lines of code. In this example we recommended Netflix movies - but these engines have huge potential in other use cases - marketing campaigns or product recommendations spring to mind. Grab your Snowflake data and get exploring!"],"metadata":{}}],"metadata":{"name":"SF-Spark-ML","notebookId":1662192302185298},"nbformat":4,"nbformat_minor":0}
